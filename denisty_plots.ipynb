{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7206935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "\n",
    "from datasets import (\n",
    "    CentralBananaDataset, MoonWithScatteringsDataset, MoonWithTwoCiclesBoundedDataset,\n",
    "    MoonWithTwoCirclesUnboundedDataset, SwissRollDataset, GMMDataset\n",
    ")\n",
    "from diffusion import ConditionalDenseModel\n",
    "from functions import make_beta_schedule\n",
    "from diffusion.ddpm_norms import DDPM as ddpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81307ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Density utility functions\n",
    "def knn_radii(real_features: np.ndarray, query_features: np.ndarray, k: int, leave_one_out: bool = False) -> np.ndarray:\n",
    "    n_neighbors = k + 1 if leave_one_out else k\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors, n_jobs=-1).fit(real_features)\n",
    "    dists, _ = nbrs.kneighbors(query_features)\n",
    "    return dists[:, -1]\n",
    "\n",
    "def get_asymmetric_bins_radii(radii: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Create dynamic bins for radii based on percentiles, consistent with counts binning.\"\"\"\n",
    "    lo, hi = float(np.min(radii)), float(np.max(radii))\n",
    "    if not np.isfinite(hi) or hi <= lo:\n",
    "        hi = lo + 1e-6\n",
    "    percentiles = np.percentile(radii, [80, 90, 97])\n",
    "    bins = [lo, percentiles[0], percentiles[1], percentiles[2], hi]\n",
    "    bins = np.maximum.accumulate(bins)\n",
    "    if len(np.unique(bins)) < 5:\n",
    "        bins = np.linspace(lo, hi, 5)\n",
    "    return np.array(bins, dtype=float)\n",
    "\n",
    "def get_asymmetric_bins_counts(counts: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Create dynamic bins for point counts based on percentiles.\"\"\"\n",
    "    lo, hi = 0, int(np.max(counts))\n",
    "    if hi <= lo:\n",
    "        hi = lo + 1\n",
    "    percentiles = np.percentile(counts, [5, 10, 20])\n",
    "    bins = [lo, percentiles[0], percentiles[1], percentiles[2], hi]\n",
    "    bins = np.maximum.accumulate(bins)\n",
    "    if len(np.unique(bins)) < 5:\n",
    "        bins = np.linspace(lo, hi, 5)\n",
    "    return np.array(bins, dtype=float)\n",
    "\n",
    "def count_points_within_radius(real_features: np.ndarray, fake_features: np.ndarray, nearest_k: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    For each point in the fake dataset, calculate the count of real points within the k-NN radius.\n",
    "    \"\"\"\n",
    "    real_nbrs = NearestNeighbors(n_neighbors=nearest_k + 1, n_jobs=-1).fit(real_features)\n",
    "    real_dists, _ = real_nbrs.kneighbors(real_features)\n",
    "    kth_nearest_radius = real_dists[:, -1]\n",
    "    fake_nbrs = NearestNeighbors(n_neighbors=len(real_features), n_jobs=-1).fit(real_features)\n",
    "    dists, _ = fake_nbrs.kneighbors(fake_features)\n",
    "    counts = np.sum(dists <= kth_nearest_radius[None, :], axis=1)\n",
    "    return counts, kth_nearest_radius\n",
    "\n",
    "def plot_color_coded_points(data: np.ndarray, values: np.ndarray, save_path: str, is_generated: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Plot color-coded scatter points for training (radii) or generated (counts) data.\n",
    "    \"\"\"\n",
    "    if is_generated:\n",
    "        bins = get_asymmetric_bins_counts(values)\n",
    "        colors = ['blue', 'lightblue', 'lightcoral', 'red']\n",
    "        label = 'Points within k-NN radius'\n",
    "    else:\n",
    "        bins = get_asymmetric_bins_radii(values)\n",
    "        colors = ['red', 'lightcoral', 'lightblue', 'blue']\n",
    "        label = 'k-NN radius'\n",
    "\n",
    "    cmap = ListedColormap(colors)\n",
    "    norm = BoundaryNorm(bins, cmap.N, clip=True)\n",
    "\n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[0.9, 0.05], wspace=0.3)\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "    if is_generated:\n",
    "        ax.scatter(data[:, 0], data[:, 1], c=values, cmap=cmap, norm=norm, s=25, edgecolor='k')\n",
    "    else:\n",
    "        ax.scatter(data[:, 0], data[:, 1], c=values, cmap=cmap, norm=norm, s=25)\n",
    "    if is_generated:\n",
    "        ax.set_xlim([-1.1, 1.1])\n",
    "        ax.set_ylim([-1.1, 1.1])\n",
    "\n",
    "    ax.set_xlabel(' ')\n",
    "    ax.set_ylabel(' ')\n",
    "    ax.set_title(' ')\n",
    "\n",
    "    cax = fig.add_subplot(gs[0, 1])\n",
    "    plt.colorbar(ScalarMappable(norm=norm, cmap=cmap), cax=cax, label=label)\n",
    "    plt.subplots_adjust(left=0.1, right=0.85, top=0.9, bottom=0.1)\n",
    "    plt.savefig(save_path, dpi=1200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "# Dataset and model utility functions\n",
    "def load_dataset(dataset_name, num_samples, batch_size, random_state):\n",
    "    dataset_classes = {\n",
    "        \"Central_Banana\": CentralBananaDataset,\n",
    "        \"Moon_with_scatterings\": MoonWithScatteringsDataset,\n",
    "        \"Moon_with_two_circles_bounded\": MoonWithTwoCiclesBoundedDataset,\n",
    "        \"Moon_with_two_circles_unbounded\": MoonWithTwoCirclesUnboundedDataset,\n",
    "        \"Swiss_Roll\": SwissRollDataset,\n",
    "    }\n",
    "    ds = dataset_classes[dataset_name](num_samples, random_state)\n",
    "    X = ds.generate()\n",
    "    X_train, _ = train_test_split(X, test_size=0.2, random_state=random_state)\n",
    "    X_output = torch.tensor(X_train, dtype=torch.float32)\n",
    "    dl = DataLoader(\n",
    "        TensorDataset(X_output),\n",
    "        batch_size=batch_size,\n",
    "        drop_last=True,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return ds, dl, X_output, X\n",
    "\n",
    "def create_model(reg, schedule_type, learning_rate):\n",
    "    eps_model = ConditionalDenseModel([2, 128, 128, 128, 2], activation=\"relu\", embed_dim=128)\n",
    "    diffusion_steps = 200\n",
    "    betas = make_beta_schedule(num_steps=diffusion_steps, mode=schedule_type, beta_range=(1e-04, 0.02))\n",
    "    return ddpm(eps_model=eps_model, betas=betas, criterion=\"mse\", lr=learning_rate, reg=reg)\n",
    "\n",
    "def train(model, train_loader, device, loss_weighting_type, steps, dataset_name):\n",
    "    model.to(device).train()\n",
    "    step = 0\n",
    "    data_iter = iter(train_loader)\n",
    "    pbar = tqdm(total=steps, desc=\"Training\")\n",
    "    while step < steps:\n",
    "        try:\n",
    "            batch = next(data_iter)\n",
    "        except StopIteration:\n",
    "            data_iter = iter(train_loader)\n",
    "            batch = next(data_iter)\n",
    "        loss, _, _ = model.train_step(batch[0].to(device, non_blocking=True), loss_weighting_type)\n",
    "        step += 1\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fbe8bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main execution with hardcoded variables (replacing argparse)\n",
    "steps = 20000\n",
    "seed = 42\n",
    "gpu_id = 0\n",
    "num_samples = 10000\n",
    "batch_size = 512\n",
    "schedule = \"linear\"\n",
    "lr = 1e-3\n",
    "loss_weighting_type = \"constant\"\n",
    "runs = 1\n",
    "nearest_k = 5\n",
    "\n",
    "device = f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\"\n",
    "reg_values = [0.0]\n",
    "datasets = [\n",
    "    \"Central_Banana\",\n",
    "    \"Moon_with_scatterings\",\n",
    "    \"Moon_with_two_circles_bounded\",\n",
    "    \"Moon_with_two_circles_unbounded\",\n",
    "    \"Swiss_Roll\",\n",
    "]\n",
    "\n",
    "main_log_dir = f\"logs/density_plots_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "os.makedirs(main_log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fceca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in datasets:\n",
    "    dataset_log_dir = os.path.join(main_log_dir, dataset_name)\n",
    "    os.makedirs(dataset_log_dir, exist_ok=True)\n",
    "\n",
    "    ds, train_loader, X_train, X_full = load_dataset(dataset_name, num_samples, batch_size, seed)\n",
    "    X_tensor = torch.tensor(X_full, dtype=torch.float32).to(device)\n",
    "\n",
    "    radii_train = knn_radii(X_full, X_full, nearest_k, leave_one_out=True)\n",
    "    plot_color_coded_points(X_full, radii_train,\n",
    "                            save_path=os.path.join(dataset_log_dir, f\"{dataset_name}_training_density.png\"),\n",
    "                            is_generated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5941ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Dataset: Central_Banana | Run 1/3 | reg=0.0 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 374/20000 [00:18<12:42, 25.74it/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 116\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, device, loss_weighting_type, steps, dataset_name)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m/storage/scratch1/e17-kadsap-diffusion-models/miniconda3/envs/diffusion/lib/python3.10/site-packages/torch/utils/data/dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/storage/scratch1/e17-kadsap-diffusion-models/miniconda3/envs/diffusion/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1481\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1480\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown_workers()\n\u001b[0;32m-> 1481\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m \n\u001b[1;32m   1485\u001b[0m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mruns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | reg=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model(reg, schedule, lr)\n\u001b[0;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weighting_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     23\u001b[0m x_gen \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(sample_shape\u001b[38;5;241m=\u001b[39mX_tensor[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, num_samples\u001b[38;5;241m=\u001b[39mnum_samples)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[6], line 118\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, device, loss_weighting_type, steps, dataset_name)\u001b[0m\n\u001b[1;32m    116\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_iter)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     data_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_iter)\n\u001b[1;32m    120\u001b[0m loss, _, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain_step(batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), loss_weighting_type)\n",
      "File \u001b[0;32m/storage/scratch1/e17-kadsap-diffusion-models/miniconda3/envs/diffusion/lib/python3.10/site-packages/torch/utils/data/dataloader.py:494\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/scratch1/e17-kadsap-diffusion-models/miniconda3/envs/diffusion/lib/python3.10/site-packages/torch/utils/data/dataloader.py:427\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/scratch1/e17-kadsap-diffusion-models/miniconda3/envs/diffusion/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1136\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_pids_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers_done_event \u001b[38;5;241m=\u001b[39m \u001b[43mmultiprocessing_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEvent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/storage/scratch1/e17-kadsap-diffusion-models/miniconda3/envs/diffusion/lib/python3.10/multiprocessing/context.py:93\u001b[0m, in \u001b[0;36mBaseContext.Event\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Returns an event object'''\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msynchronize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Event\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEvent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/scratch1/e17-kadsap-diffusion-models/miniconda3/envs/diffusion/lib/python3.10/multiprocessing/synchronize.py:324\u001b[0m, in \u001b[0;36mEvent.__init__\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, ctx):\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond \u001b[38;5;241m=\u001b[39m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCondition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mSemaphore(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/storage/scratch1/e17-kadsap-diffusion-models/miniconda3/envs/diffusion/lib/python3.10/multiprocessing/context.py:78\u001b[0m, in \u001b[0;36mBaseContext.Condition\u001b[0;34m(self, lock)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Returns a condition object'''\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msynchronize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Condition\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCondition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/scratch1/e17-kadsap-diffusion-models/miniconda3/envs/diffusion/lib/python3.10/multiprocessing/synchronize.py:215\u001b[0m, in \u001b[0;36mCondition.__init__\u001b[0;34m(self, lock, ctx)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock \u001b[38;5;241m=\u001b[39m lock \u001b[38;5;129;01mor\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mRLock()\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sleeping_count \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mSemaphore(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_woken_count \u001b[38;5;241m=\u001b[39m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSemaphore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_semaphore \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mSemaphore(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_methods()\n",
      "File \u001b[0;32m/storage/scratch1/e17-kadsap-diffusion-models/miniconda3/envs/diffusion/lib/python3.10/multiprocessing/context.py:83\u001b[0m, in \u001b[0;36mBaseContext.Semaphore\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Returns a semaphore object'''\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msynchronize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Semaphore\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSemaphore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/scratch1/e17-kadsap-diffusion-models/miniconda3/envs/diffusion/lib/python3.10/multiprocessing/synchronize.py:126\u001b[0m, in \u001b[0;36mSemaphore.__init__\u001b[0;34m(self, value, ctx)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m, ctx):\n\u001b[0;32m--> 126\u001b[0m     \u001b[43mSemLock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSEMAPHORE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSEM_VALUE_MAX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/scratch1/e17-kadsap-diffusion-models/miniconda3/envs/diffusion/lib/python3.10/multiprocessing/synchronize.py:57\u001b[0m, in \u001b[0;36mSemLock.__init__\u001b[0;34m(self, kind, value, maxvalue, ctx)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m         sl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_semlock \u001b[38;5;241m=\u001b[39m \u001b[43m_multiprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSemLock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[43munlink_now\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 375/20000 [00:29<12:42, 25.74it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "for dataset_name in datasets:\n",
    "    dataset_log_dir = os.path.join(main_log_dir, dataset_name)\n",
    "    os.makedirs(dataset_log_dir, exist_ok=True)\n",
    "\n",
    "    ds, train_loader, X_train, X_full = load_dataset(dataset_name, num_samples, batch_size, seed)\n",
    "    X_tensor = torch.tensor(X_full, dtype=torch.float32).to(device)\n",
    "\n",
    "    radii_train = knn_radii(X_full, X_full, nearest_k, leave_one_out=True)\n",
    "    plot_color_coded_points(X_full, radii_train,\n",
    "                            save_path=os.path.join(dataset_log_dir, f\"{dataset_name}_training_density.png\"),\n",
    "                            is_generated=False)\n",
    "\n",
    "    for reg in reg_values:\n",
    "        reg_log_dir = os.path.join(dataset_log_dir, f\"reg_{reg}\")\n",
    "        os.makedirs(reg_log_dir, exist_ok=True)\n",
    "\n",
    "        for run_idx in range(runs):\n",
    "            print(f\"\\n--- Dataset: {dataset_name} | Run {run_idx+1}/{runs} | reg={reg} ---\")\n",
    "            model = create_model(reg, schedule, lr)\n",
    "            model = train(model, train_loader, device, loss_weighting_type, steps, dataset_name)\n",
    "            model.eval()\n",
    "\n",
    "            x_gen = model.generate(sample_shape=X_tensor[0].shape, num_samples=num_samples)[0].cpu().numpy()\n",
    "\n",
    "            counts_gen, _ = count_points_within_radius(X_full, x_gen, nearest_k)\n",
    "            plot_color_coded_points(x_gen, counts_gen,\n",
    "                                    save_path=os.path.join(reg_log_dir, f\"{dataset_name}_generated_run_{run_idx+1}.png\"),\n",
    "                                    is_generated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c2059a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
